{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최근 영업일 데이터 뽑기\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://finance.naver.com/sise/sise_deposit.nhn'\n",
    "data = rq.get(url)\n",
    "data_html = BeautifulSoup(data.content)\n",
    "parse_day = data_html.select_one(\n",
    "    'div.subtop_sise_graph2 > ul.subtop_chart_note > li > span.tah').text   # select_one - 해당 데이터 태그 추출\n",
    "\n",
    "print(parse_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "biz_day = re.findall('[0-9]+', parse_day)   # 숫자에 해당하는 부분 추출\n",
    "biz_day = ''.join(biz_day)  # 숫자 합쳐주기\n",
    "\n",
    "print(biz_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 한국거래소 업종분류 현황 및 개별지표 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201에서 CSV 데이터를 받아오기 위한 OTP 받아오기\n",
    "\n",
    "import requests as rq\n",
    "import cloudscraper     # 보안서비스 우회 요청 처리 라이브러리\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "gen_otp_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'\n",
    "gen_otp_stk = {\n",
    "    'locale':'ko-KR',\n",
    "    'mktId': 'STK', # 코스피 종목\n",
    "    'trdDd': biz_day,\n",
    "    'money':'1',\n",
    "    'csvxls_isNo':'false',\n",
    "    'name':'fileDown',\n",
    "    'url':'dbms/MDC/STAT/standard/MDCSTAT03901'\n",
    "}\n",
    "# headers = {'Referer':'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0201'}   # Referer: 링크를 통해 각각의 웹사이트로 방문할 때 남는 흔적, 이 흔적이 없으면 서버가 로봇으로 인식해 정보를 주지 않음\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "response = scraper.post(gen_otp_url, params=gen_otp_stk)    # 포스트 요청 - cloudscraper 사용하여 요청 우회\n",
    "\n",
    "otp_stk = response.text\n",
    "\n",
    "print(otp_stk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 과정을 거처 생성된 OTP를 제출 시, 원하는 데이터 다운 가능\n",
    "\n",
    "down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'\n",
    "down_sector_stk = scraper.post(down_url, params={'code': otp_stk})              # cloudscraper 사용\n",
    "sector_stk = pd.read_csv(BytesIO(down_sector_stk.content), encoding='EUC-KR')   # BytesIO : 받은 데이터의 content 부분을 바이너리 스트림으로 만든 후 read_csv 함수를 통해 데이터 읽어오기\n",
    "\n",
    "sector_stk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코스닥 시작 데이터도 동일 과정을 통해 다운\n",
    "\n",
    "gen_otp_ksq = {\n",
    "    'locale':'ko-KR',\n",
    "    'mktId': 'KSQ', # 코스피 종목\n",
    "    'trdDd': biz_day,\n",
    "    'money':'1',\n",
    "    'csvxls_isNo':'false',\n",
    "    'name':'fileDown',\n",
    "    'url':'dbms/MDC/STAT/standard/MDCSTAT03901'\n",
    "}\n",
    "\n",
    "otp_ksq = scraper.post(gen_otp_url, params=gen_otp_ksq).text\n",
    "\n",
    "down_sector_ksq = scraper.post(down_url, params={'code': otp_ksq})\n",
    "sector_ksq = pd.read_csv(BytesIO(down_sector_ksq.content), encoding='EUC-KR')\n",
    "\n",
    "sector_ksq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코스피, 코스닥 데이터 합치기\n",
    "\n",
    "krx_sector = pd.concat([sector_stk, sector_ksq]).reset_index(drop=True)     # 두 데이터 합쳐주고 reset_index()로 인덱스 리셋, drop=True로 인덱스로 세팅한 열을 삭제\n",
    "krx_sector['종목명'] = krx_sector['종목명'].str.strip()     # 종목명에 공백이 있는 경우 삭제\n",
    "krx_sector['기준일'] = biz_day\n",
    "\n",
    "krx_sector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.2 개별종목 지표 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별지표 OTP 생성 후 데이터 불러오기\n",
    "\n",
    "import cloudscraper     # 보안서비스 우회 요청 처리 라이브러리\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "gen_otp_url = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'\n",
    "gen_otp_data = {\n",
    "    'searchType':'1',\n",
    "    'mktId':'ALL',\n",
    "    'trdDd':biz_day,\n",
    "    'csvxls_isNo':'false',\n",
    "    'name':'fileDown',\n",
    "    'url':'dbms/MDC/STAT/standard/MDCSTAT03501'\n",
    "}\n",
    "# headers={'Referer':'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader'}\n",
    "scraper = cloudscraper.create_scraper()\n",
    "otp = scraper.post(gen_otp_url, params=gen_otp_data).text\n",
    "\n",
    "down_url = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'\n",
    "krx_ind = scraper.post(down_url, params={'code': otp})\n",
    "\n",
    "krx_ind = pd.read_csv(BytesIO(krx_ind.content), encoding='EUC-KR')\n",
    "krx_ind['종목명'] = krx_ind['종목명'].str.strip()   # 종목명 없는 항목은 삭제\n",
    "krx_ind['기준일'] = biz_day\n",
    "\n",
    "krx_ind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.3 데이터 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(set(krx_sector['종목명']).symmetric_difference(set(krx_ind['종목명']))) # 대칭 차집합을 구하는 함수 / 대칭 차집합 : 두 집합의 합집합에서 공통 부분만 제외한 집합 - 한 집합에만 존재하는 요소들의 집합\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(): 두 개의 데이터프레임을 병합할 때 사용하는 함수, SQL의 JOIN 연산과 유사한 방식으로 데이터 결합\n",
    "kor_ticker = pd.merge(krx_sector,   # 왼쪽 데이터프레임\n",
    "                      krx_ind,      # 오른쪽 데이터프레임\n",
    "                      on=krx_sector.columns.intersection(   # on=\"두 데이터프레임에서 공통으로 사용할 열 이름. 열 이름이 같을 경우에만 사용.\"\n",
    "                          krx_ind.columns).tolist(),\n",
    "                      how='outer')   # 병합 방식(outer:합집합)\n",
    "\n",
    "kor_ticker.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 종목과 스팩, 우선주, 리츠, 기타 주식을 구분한다.\n",
    "\n",
    "스팩(Special Purpose Acquisition Company, SPAC): 기업인수를 목적으로 하는 페이퍼컴퍼니\n",
    "    - 대부분 증권사 주관으로 설립되며, 스팩이 먼저 투자자들의 자금을 모아 주식 시장에 상장이 되고 나면, 그 이후에 괜찮은 비상장기업을 찾아 합병하는 방식으로 최종 기업 인수가 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas의 []: [] 내부의 조건이 True인 항목들을 뽑아낸다.\n",
    "\n",
    "print(kor_ticker[kor_ticker['종목명'].str.contains('스팩|제[0-9]+호')]['종목명'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목코드 끝이 0이 아닌 종목은 우선주에 해당, 우선주를 뽑는 코드\n",
    "\n",
    "print(kor_ticker[kor_ticker['종목코드'].str[-1:] != '0']['종목명'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리츠 종목은 종목명이 리츠로 끝남\n",
    "\n",
    "print(kor_ticker[kor_ticker['종목명'].str.endswith('리츠')]['종목명'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.where(condition, val_if_true, val_if_false): 조건이 참이면 val_if_true를, 거짓이면 val_if_false를 반환\n",
    "# 거짓에 np.where를 중첩으로 넣는 경우 - 거짓일 경우에 중첩된 np.where()을 실행\n",
    "\n",
    "kor_ticker['종목구분'] = np.where(kor_ticker['종목명'].str.contains('스팩|재[0-9]+호'), '스팩',\n",
    "                            np.where(kor_ticker['종목코드'].str[-1:] != '0', '우선주',\n",
    "                                     np.where(kor_ticker['종목명'].str.endswith('리츠'), '리츠',\n",
    "                                              np.where(kor_ticker['종목명'].isin(diff), '기타',\n",
    "                                                  '보통주'))))\n",
    "\n",
    "# df.reset_index(): 데이터프레임의 인덱스를 초기화하거나 재설정하는 데 사용됨\n",
    "# 기존 인덱스를 기본 정수형 인덱스로 변환하고, 변환되기 전 인덱스를 데이터프레임의 열로 변환하거나 제거할 수 있음\n",
    "kor_ticker = kor_ticker.reset_index(drop=True)\n",
    "# 열 이름의 공백 삭제\n",
    "kor_ticker.columns = kor_ticker.columns.str.replace(' ', '')\n",
    "kor_ticker = kor_ticker[['종목코드', '종목명', '시장구분', '종가',\n",
    "                         '시가총액', '기준일', 'EPS', '선행EPS', 'BPS', '주당배당금', '종목구분']]\n",
    "# SQL에는 NaN이 입력되지 않으므로 None으로 재표기\n",
    "kor_ticker = kor_ticker.replace({np.nan: None})\n",
    "# 기준일을 datetime 형태로 변경한다\n",
    "kor_ticker['기준일'] = pd.to_datetime(kor_ticker['기준일'])\n",
    "\n",
    "kor_ticker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운로드받은 정보를 kor_ticker 테이블에 upsert 형태로 저장\n",
    "\n",
    "import pymysql\n",
    "\n",
    "con = pymysql.connect(user='root',\n",
    "                      passwd='1234',\n",
    "                      host='127.0.0.1',\n",
    "                      db='stock_db',\n",
    "                      charset='utf8')\n",
    "\n",
    "mycursor = con.cursor()\n",
    "\n",
    "query = f\"\"\"\n",
    "    insert into kor_ticker (종목코드, 종목명, 시장구분, 종가, 시가총액, 기준일, EPS, 선행EPS, BPS, 주당배당금, 종목구분)\n",
    "    values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) as new\n",
    "    on duplicate key update\n",
    "    종목명=new.종목명, 시장구분=new.시장구분, 종가=new.종가, 시가총액=new.시가총액, EPS=new.EPS, 선행EPS=new.선행EPS,\n",
    "    BPS=new.BPS, 주당배당금=new.주당배당금, 종목구분=new.종목구분;\n",
    "\"\"\"\n",
    "\n",
    "args = kor_ticker.values.tolist()\n",
    "\n",
    "mycursor.executemany(query, args)   # cursor.executemany(query, args): 같은 SQL문을 여러번 반복적으로 실행할 때 사용 / args는 삽입할 다수의 데이터셋의 모임\n",
    "\n",
    "\n",
    "con.commit()    # 데이터베이스 변경사항을 실제 db에 저장\n",
    "con.close()     # 데이터베이스와의 연결을 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 WICS 기준 섹터 정보 크롤링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "\n",
    "url = f\"https://www.wiseindex.com/Index/GetIndexComponets?ceil_yn=0&dt={biz_day}&sec_cd=G10\"        # json 파일으로 표현된 데이터\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "data = scraper.get(url=url).json()\n",
    "\n",
    "type(data)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list: 해당 섹터의 구성종목 정보가 포함되어 있음\n",
    "data['list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector: 각종 섹터의 코드 정보가 포함\n",
    "data['sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 부분 데이터를 데이터프레임 형태로 변경\n",
    "\n",
    "data_pd = pd.json_normalize(data['list'])\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 섹터의 구성종목을 뽑기 - sec_cd에 해당하는 부분을 for문으로 변경 후 추출\n",
    "\n",
    "import time\n",
    "import json\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sector_code = [\n",
    "    'G25', 'G35', 'G50', 'G40', 'G10', 'G20', 'G55', 'G30', 'G15', 'G45'\n",
    "]\n",
    "\n",
    "data_sector = []\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "for i in tqdm(sector_code):\n",
    "    url = f\"https://www.wiseindex.com/Index/GetIndexComponets?ceil_yn=0&dt={biz_day}&sec_cd={i}\"\n",
    "    data = scraper.get(url=url).json()\n",
    "    data_pd = pd.json_normalize(data['list'])\n",
    "    \n",
    "    data_sector.append(data_pd)\n",
    "    time.sleep(2)\n",
    "    \n",
    "kor_sector = pd.concat(data_sector, axis=0) # axis=0: 행 방향으로 합치기\n",
    "kor_sector = kor_sector[['IDX_CD', 'CMP_CD', 'CMP_KOR', 'SEC_NM_KOR']]\n",
    "kor_sector['기준일'] = biz_day\n",
    "kor_sector['기준일'] = pd.to_datetime(kor_sector['기준일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_sector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db에 위 정보를 저장\n",
    "\n",
    "import pymysql\n",
    "\n",
    "con = pymysql.connect(user='root',\n",
    "                      passwd='1234',\n",
    "                      host='127.0.0.1',\n",
    "                      db='stock_db',\n",
    "                      charset='utf8')\n",
    "\n",
    "mycursor = con.cursor()\n",
    "\n",
    "# upsert 형식 : update(기존 값 존재 시) + insert(기존 값 없을 때)\n",
    "query = \"\"\"\n",
    "    insert into kor_sector(IDX_CD, CMP_CD, CMP_KOR, SEC_NM_KOR, 기준일)\n",
    "    values (%s, %s, %s, %s, %s) as new\n",
    "    on duplicate key update\n",
    "    IDX_CD=new.IDX_CD, CMP_CD=new.CMP_CD, CMP_KOR=new.CMP_KOR, SEC_NM_KOR=new.SEC_NM_KOR\n",
    "\"\"\"\n",
    "args = kor_sector.values.tolist()\n",
    "\n",
    "mycursor.executemany(query, args)\n",
    "con.commit()\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.4 수정주가 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.1 개별종목 주가 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB에서 티커 데이터를 불러오기\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:1234@127.0.0.1:3306/stock_db')\n",
    "# 가장 최근 일자 & 보통주에 해당하는 종목 선택\n",
    "query = \"\"\"\n",
    "    select * from kor_ticker\n",
    "    where 기준일 = (select max(기준일) from kor_ticker)\n",
    "        and 종목구분 = '보통주';\n",
    "\"\"\"\n",
    "ticker_list = pd.read_sql(query, con=engine)\n",
    "engine.dispose()\n",
    "\n",
    "ticker_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주가 데이터 페이지 크롤링\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import cloudscraper\n",
    "from io import BytesIO\n",
    "from datetime import date\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "i = 0       # 향후 for문을 통해 모든 종목의 주가를 크롤링하기 위한 변수 설정\n",
    "ticker = ticker_list['종목코드'][i] # 원하는 종목의 티커를 설정\n",
    "fr = (date.today() + relativedelta(years=-5)).strftime('%Y%m%d')    # 현재 날짜에서 원하는 기간만큼을 뺀 후 yyyymmdd형식으로 만들기 - relativedelta(): 원하는 기간만큼 빼거나 더해주는 함수, strftime(): 시간 형식으로 바꿔주는 함수\n",
    "to = (date.today()).strftime('%Y%m%d')\n",
    "\n",
    "url = f\"https://m.stock.naver.com/front-api/external/chart/domestic/info?symbol={ticker}&requestType=1&startTime={fr}&endTime={to}&timeframe=day\"\n",
    "\n",
    "data = scraper.get(url=url).content\n",
    "data_price = pd.read_csv(BytesIO(data))\n",
    "\n",
    "data_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클렌징 작업 수행\n",
    "\n",
    "import re\n",
    "\n",
    "price = data_price.iloc[:, 0:6] # 모든 행, 0번째에서 5번째까지의 열에 해당하는 dataframe 추출\n",
    "price.columns = ['날짜', '시가', '고가', '저가', '종가', '거래량']\n",
    "price = price.dropna()\n",
    "price['날짜'] = price['날짜'].str.extract('(\\d+)')  # 날짜 열에서 숫자만을 추출\n",
    "price['날짜'] = pd.to_datetime(price['날짜']) \n",
    "price['종목코드'] = ticker\n",
    "\n",
    "price.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.2 전 종목 주가 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import cloudscraper\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "\n",
    "import traceback    # 예외처리 상세 내역 출력 라이브러리\n",
    "# DB 연결\n",
    "engine = create_engine('mysql+pymysql://root:1234@127.0.0.1:3306/stock_db')\n",
    "con = pymysql.connect(user='root',\n",
    "                      passwd='1234',\n",
    "                      host='127.0.0.1',\n",
    "                      db='stock_db',\n",
    "                      charset='utf8')\n",
    "\n",
    "mycursor = con.cursor()\n",
    "\n",
    "# 티커 리스트 불러오기\n",
    "ticker_list = pd.read_sql(\n",
    "    \"\"\"\n",
    "    select * from kor_ticker\n",
    "    where 기준일 = (select max(기준일) from kor_ticker)\n",
    "        and 종목구분 = '보통주';\n",
    "    \"\"\", con=engine\n",
    ")\n",
    "\n",
    "# DB 저장 쿼리\n",
    "query = \"\"\"\n",
    "    insert into kor_price(날짜, 시가, 고가, 저가, 종가, 거래량, 종목코드)\n",
    "    values (%s, %s, %s, %s, %s, %s, %s) as new\n",
    "    on duplicate key update\n",
    "    시가=new.시가, 고가=new.고가, 저가=new.저가, 종가=new.종가, 거래량=new.거래량;\n",
    "\"\"\"\n",
    "\n",
    "# 오류 발생 시 저장할 리스트 생성\n",
    "error_list = []\n",
    "\n",
    "# 전 종목 주가 다운로드 및 저장\n",
    "for i in tqdm(range(0, len(ticker_list))):\n",
    "    \n",
    "    # 티커 선택\n",
    "    ticker = ticker_list['종목코드'][i]\n",
    "    \n",
    "    # 시작일과 종료일\n",
    "    fr = (date.today() + relativedelta(years=-5)).strftime('%Y%m%d')    # 현재 날짜에서 원하는 기간만큼을 뺀 후 yyyymmdd형식으로 만들기 - relativedelta(): 원하는 기간만큼 빼거나 더해주는 함수, strftime(): 시간 형식으로 바꿔주는 함수\n",
    "    to = (date.today()).strftime('%Y%m%d')\n",
    "    \n",
    "    # 오류 발생 시 이를 무시하고 다음 루프로 진행\n",
    "    try:\n",
    "        \n",
    "        # url 생성\n",
    "        url = f'https://m.stock.naver.com/front-api/external/chart/domestic/info?symbol={ticker}&requestType=1&startTime={fr}&endTime={to}&timeframe=day'\n",
    "        \n",
    "        # 데이터 다운로드\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        data = scraper.get(url).content\n",
    "        data_price = pd.read_csv(BytesIO(data))\n",
    "        \n",
    "        # 데이터 클렌징\n",
    "        price = data_price.iloc[:, 0:6] # 모든 행, 0번째에서 5번째까지의 열에 해당하는 dataframe 추출\n",
    "        price.columns = ['날짜', '시가', '고가', '저가', '종가', '거래량']\n",
    "        price = price.dropna()\n",
    "        price['날짜'] = price['날짜'].str.extract('(\\d+)')  # 날짜 열에서 숫자만을 추출\n",
    "        price['날짜'] = pd.to_datetime(price['날짜']) \n",
    "        price['종목코드'] = ticker\n",
    "        \n",
    "        # 주가 데이터를 DB에 저장\n",
    "        args = price.values.tolist()\n",
    "        # print(args)\n",
    "        mycursor.executemany(query, args)\n",
    "        con.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"예외 발생: {e}\")\n",
    "        # traceback.print_exc()\n",
    "        # break\n",
    "        # 오류 발생 시 error_list에 티커 저장하고 넘어가기\n",
    "        print(ticker)\n",
    "        error_list.append(ticker)\n",
    "        \n",
    "    # 타임슬립 적용\n",
    "    time.sleep(2)\n",
    "\n",
    "# DB 연결 종료\n",
    "engine.dispose()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.5 재무제표 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5.1 재무제표 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표 형태로 제공되는 재무제표 항목들을 read_html() 함수로 추출\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('mysql+pymysql://root:1234@127.0.0.1:3306/stock_db')\n",
    "query = \"\"\"\n",
    "    select * from kor_ticker\n",
    "    where 기준일 = (select max(기준일) from kor_ticker)\n",
    "        and 종목구분 = '보통주'\n",
    "\"\"\"\n",
    "ticker_list = pd.read_sql(query, con=engine)\n",
    "engine.dispose()\n",
    "\n",
    "i = 0\n",
    "ticker = ticker_list['종목코드'][i]\n",
    "\n",
    "url = f'https://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&gicode=A{ticker}'\n",
    "data = pd.read_html(url, displayed_only=False)\n",
    "\n",
    "[item.head(3) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "위에서 가져온 재무제표 표 내역\n",
    "0: 포괄손익계산서(연간)\n",
    "1: 포괄손익계산서(분기)\n",
    "2: 재무상태표(연간)\n",
    "3: 재무상태표(분기)\n",
    "4: 현금흐름표(연간)\n",
    "5: 현금흐름표(분기)\n",
    "\"\"\"\n",
    "\n",
    "# 연간 기준 포괄손익계산서, 재무상태표, 현금흐름표의 열 이름 살펴보기\n",
    "print(\n",
    "    data[0].columns.to_list(), '\\n',\n",
    "    data[2].columns.to_list(), '\\n',\n",
    "    data[3].columns.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요하지 않은 전연동기, 전년동기(%) 열 삭제\n",
    "data_fs_y = pd.concat(\n",
    "    [data[0].iloc[:, ~data[0].columns.str.contains('전년동기')], data[2], data[4]])\n",
    "data_fs_y = data_fs_y.rename(columns={data_fs_y.columns[0]: \"계정\"}) # 첫 번째 열 이름을 \"계정\" 으로 바꾸기\n",
    "\n",
    "data_fs_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연간 재무제표에 해당하는 열만을 선택해야 함. 먼저, 결산월에 해당하는 데이터를 크롤링\n",
    "\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "page_data = scraper.get(url)\n",
    "page_data_html = BeautifulSoup(page_data.content, 'html.parser')\n",
    "\n",
    "fiscal_data = page_data_html.select('div.corp_group1 > h2')\n",
    "fiscal_data_text = fiscal_data[1].text  # 결산 데이터는 2번째에 있음\n",
    "fiscal_data_text = re.findall('[0-9]+', fiscal_data_text)\n",
    "\n",
    "print(fiscal_data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연간 재무제표에 해당하는 열만 선택\n",
    "\n",
    "print(data_fs_y.columns.str[-2:])\n",
    "data_fs_y = data_fs_y.loc[:, (data_fs_y.columns == \"계정\") |\n",
    "                          (data_fs_y.columns.str[-2:].isin(fiscal_data_text))]  # [-2:]: 열의 글자 인덱스 -2, -1을 불러옴\n",
    "\n",
    "data_fs_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 연도의 데이터가 NaN인 항목을 찾아 삭제\n",
    "\n",
    "data_fs_y[data_fs_y.loc[:, ~data_fs_y.columns.isin(['계정'])].isna().all(axis=1)].head()        # .all(axis=1): 열 방향(가로)로 체크했을 때 모든 값이 True 인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일 계정명이 여러 번 반복됨을 확인 함, 하나만 남겨놓는 작업 필요\n",
    "data_fs_y['계정'].value_counts(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 두 내용을 포함하여 클렌징이 필요한 내용들을 함수로 구성\n",
    "\n",
    "def clean_fs(df, ticker, frequency):\n",
    "    df = df[~df.loc[:, ~df.columns.isin(['계정'])].isna().all(axis=1)]  # 모든 연도 데이터가 NaN인 항목 제외\n",
    "    df = df.drop_duplicates(['계정'], keep='first')     # 계정명 중복 시, 첫 번째 위치하는 데이터만 남김\n",
    "    df = pd.melt(df, id_vars='계정', var_name='기준일', value_name='값')    # 열로 긴 데이터를 행으로 긴 데이터로 변경\n",
    "    df = df[~pd.isnull(df['값'])]   # 계정값이 없는 부분 삭제\n",
    "    df['계정'] = df['계정'].replace({'계산에 참여한 계정 펼치기': ''}, regex=True)  # 계산에 참여한 계정 펼치기라는 글자는 페이지의 +에 해당하는 부분 -> replace로 제거\n",
    "    df['기준일'] = pd.to_datetime(df['기준일'], format='%Y/%m') + pd.tseries.offsets.MonthEnd() # to_datetime() 메서드로 %Y/%m 형식인 데이터를 YYYY-mm 형태로 바꾼 후 MonthEnd()로 월말에 해당하는 일자 붙이기\n",
    "    \n",
    "    df['종목코드'] = ticker\n",
    "    df['공시구분'] = frequency\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fs_y_clean = clean_fs(data_fs_y, ticker, 'y')\n",
    "\n",
    "data_fs_y_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분기 재무제표도 클랜징\n",
    "\n",
    "data_fs_q = pd.concat(\n",
    "    [data[1].iloc[:, ~data[1].columns.str.contains('전년동기')], data[3], data[5]])\n",
    "data_fs_q = data_fs_q.rename(columns={data_fs_q.columns[0]: '계정'})\n",
    "data_fs_q_clean = clean_fs(data_fs_q, ticker, 'q')\n",
    "\n",
    "data_fs_q_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 테이블 하나로 묶기\n",
    "data_fs_bind = pd.concat([data_fs_y_clean, data_fs_q_clean])\n",
    "\n",
    "data_fs_bind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5.2 전 종목 재무제표 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전 종목 재무제표를 DB에 저장\n",
    "\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# DB 연결\n",
    "engine = create_engine('mysql+pymysql://root:1234@127.0.0.1:3306/stock_db')\n",
    "con = pymysql.connect(user='root',\n",
    "                      passwd='1234',\n",
    "                      host='127.0.0.1',\n",
    "                      db='stock_db',\n",
    "                      charset='utf8')\n",
    "mycursor = con.cursor()\n",
    "\n",
    "# 티커 리스트 불러오기\n",
    "ticker_list = pd.read_sql(\n",
    "    \"\"\"\n",
    "    select * from kor_ticker\n",
    "    where 기준일 = (select max(기준일) from kor_ticker)\n",
    "        and 종목구분 = '보통주';\n",
    "    \"\"\", con=engine)\n",
    "\n",
    "# DB 저장 쿼리\n",
    "query = \"\"\"\n",
    "    insert into kor_fs (계정, 기준일, 값, 종목코드, 공시구분)\n",
    "    values (%s, %s, %s, %s, %s) as new\n",
    "    on duplicate key update\n",
    "    값 = new.값\n",
    "\"\"\"\n",
    "\n",
    "# 오류 발생 시 저장할 리스트 생성\n",
    "error_list = []\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# 재무제표 클렌징 함수\n",
    "def clean_fs(df, ticker, frequency):\n",
    "    \n",
    "    df = df[~df.loc[:, ~df.columns.isin(['계정'])].isna().all(axis=1)]  # 모든 연도 데이터가 NaN인 항목 제외\n",
    "    df = df.drop_duplicates(['계정'], keep='first')     # 계정명 중복 시, 첫 번째 위치하는 데이터만 남김\n",
    "    df = pd.melt(df, id_vars='계정', var_name='기준일', value_name='값')    # 열로 긴 데이터를 행으로 긴 데이터로 변경\n",
    "    df = df[~pd.isnull(df['값'])]   # 계정값이 없는 부분 삭제\n",
    "    df['계정'] = df['계정'].replace({'계산에 참여한 계정 펼치기': ''}, regex=True)  # 계산에 참여한 계정 펼치기라는 글자는 페이지의 +에 해당하는 부분 -> replace로 제거\n",
    "    df['기준일'] = pd.to_datetime(df['기준일'], format='%Y/%m') + pd.tseries.offsets.MonthEnd() # to_datetime() 메서드로 %Y/%m 형식인 데이터를 YYYY-mm 형태로 바꾼 후 MonthEnd()로 월말에 해당하는 일자 붙이기\n",
    "    \n",
    "    df['종목코드'] = ticker\n",
    "    df['공시구분'] = frequency\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 전 종목에 대한 재무제표 크롤링\n",
    "for i in tqdm(range(0, len(ticker_list))):\n",
    "    \n",
    "    # 티커 선택\n",
    "    ticker = ticker_list['종목코드'][i]\n",
    "    \n",
    "    # 오류 발생 시 무시 후 다음 루프 진행\n",
    "    try:\n",
    "        # url 생성\n",
    "        url = f'https://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&gicode=A{ticker}'\n",
    "        \n",
    "        # 데이터 받아오기\n",
    "        data = pd.read_html(url, displayed_only=False)\n",
    "        \n",
    "        # 연간 데이터\n",
    "        data_fs_y = pd.concat(\n",
    "            [data[0].iloc[:, ~data[0].columns.str.contains('전년동기')], data[2], data[4]])\n",
    "        data_fs_y = data_fs_y.rename(columns={data_fs_y.columns[0]: \"계정\"}) # 첫 번째 열 이름을 \"계정\" 으로 바꾸기\n",
    "\n",
    "        # 결산년 찾기\n",
    "        page_data = scraper.get(url)\n",
    "        page_data_html = BeautifulSoup(page_data.content, 'html.parser')\n",
    "        \n",
    "        fiscal_data = page_data_html.select('div.corp_group1 > h2')\n",
    "        fiscal_data_text = fiscal_data[1].text  # 결산 데이터는 2번째에 있음\n",
    "        fiscal_data_text = re.findall('[0-9]+', fiscal_data_text)\n",
    "        \n",
    "        # 결산년에 해당하는 계정만 남기기\n",
    "        data_fs_y = data_fs_y.loc[:, (data_fs_y.columns == '계정') | \n",
    "                                  (data_fs_y.columns.str[-2:].isin(fiscal_data_text))]\n",
    "        \n",
    "        # 클렌징\n",
    "        data_fs_y_clean = clean_fs(data_fs_y, ticker, 'y')\n",
    "        \n",
    "        # 분기 데이터\n",
    "        data_fs_q = pd.concat(\n",
    "            [data[1].iloc[:, ~data[0].columns.str.contains('전년동기')], data[3], data[5]])\n",
    "        data_fs_q = data_fs_q.rename(columns={data_fs_q.columns[0]: \"계정\"})\n",
    "        \n",
    "        data_fs_q_clean = clean_fs(data_fs_q, ticker, 'q')\n",
    "        \n",
    "        # 2개 합치기\n",
    "        data_fs_bind = pd.concat([data_fs_y_clean, data_fs_q_clean])\n",
    "        \n",
    "        # 재무제표 데이터를 DB에 저장\n",
    "        args = data_fs_bind.values.tolist()\n",
    "        mycursor.executemany(query, args)\n",
    "        con.commit()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        # 오류 발생 시 해당 종목명을 저장하고 다음 루프로 이동\n",
    "        print(ticker)\n",
    "        error_list.append(ticker)\n",
    "        \n",
    "    # 타임슬립 적용\n",
    "    time.sleep(2)\n",
    "    \n",
    "# DB 연결 종료\n",
    "engine.dispose()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
